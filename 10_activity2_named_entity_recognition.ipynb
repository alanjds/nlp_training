{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: Named Entity Recognition\n",
    "\n",
    "**Named-entity recognition (NER)** (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify elements in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Most research on NER systems has been structured as taking an unannotated block of text, such as this one:\n",
    "\n",
    "Jim bought 300 shares of Acme Corp. in 2006.\n",
    "And producing an annotated block of text that highlights the names of entities:\n",
    "\n",
    "[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\n",
    "In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\n",
    "\n",
    "State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "This task aims to names of organizations/brands from the corpus.txt. There is two basic approaches:\n",
    "\n",
    "* Grammar based\n",
    "* Machine Learning with a corpus of sequential labeled examples\n",
    "\n",
    "The result should show the candidates for organization/brands retrieved from corpus.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "\n",
    "# this could be done in a iterate way for performance in huge corpus\n",
    "with codecs.open('corpus.txt', encoding='utf8') as fp:\n",
    "    corpus = fp.read()\n",
    "    \n",
    "# corpus has 55 millions characters. I am going to use just 10k to speed up a bit the things\n",
    "corpus = corpus[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1st solution: the easiest, let some toolkit do it for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import polyglot\n",
    "polyglot.data_path = '/usr/share/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "text = Text(\"A presidenta do Brasil Ã© Dilma Roussef.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I-ORG([u'Brasil']), I-PER([u'Dilma', u'Roussef'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "text = Text(corpus[:2000])\n",
    "# check with more than 2k. Polyglot has a bug with unicode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I-PER([u'Kit']),\n",
       " I-PER([u'Aro']),\n",
       " I-ORG([u'Pirelli']),\n",
       " I-ORG([u'Pirelli']),\n",
       " I-ORG([u'Pirelli']),\n",
       " I-PER([u'Safra']),\n",
       " I-LOC([u'Pa\\xeds']),\n",
       " I-LOC([u'Fran\\xe7a']),\n",
       " I-LOC([u'Vanilia'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd solution: using a grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we need to tag the corpus.\n",
    "import nlpnet\n",
    "nlpnet.set_data_dir(str('/usr/share/nlpnet_data/'))\n",
    "tagger = nlpnet.POSTagger()\n",
    "sentences = tagger.tag(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Kit', u'N'),\n",
       " (u'com', u'PREP'),\n",
       " (u'4', u'NUM'),\n",
       " (u'Pneus', u'NPROP'),\n",
       " (u'de', u'NPROP'),\n",
       " (u'Alta', u'NPROP'),\n",
       " (u'Performance', u'NPROP'),\n",
       " (u'Pirelli', u'NPROP'),\n",
       " (u'Aro', u'NPROP'),\n",
       " (u'16', u'NPROP'),\n",
       " (u'205/55R16', u'NPROP'),\n",
       " (u'Phantom', u'NPROP'),\n",
       " (u'Chegou', u'V'),\n",
       " (u'o', u'ART'),\n",
       " (u'kit', u'N'),\n",
       " (u'que', u'PRO-KS'),\n",
       " (u'junta', u'V'),\n",
       " (u'resist\\xeancia', u'N'),\n",
       " (u'e', u'KC'),\n",
       " (u'conforto', u'N'),\n",
       " (u',', u'PU'),\n",
       " (u'al\\xe9m', u'PREP'),\n",
       " (u'de', u'PREP'),\n",
       " (u'n\\xedveis', u'N'),\n",
       " (u'm\\xe1ximos', u'ADJ'),\n",
       " (u'de', u'PREP'),\n",
       " (u'seguran\\xe7a', u'N'),\n",
       " (u'.', u'PU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "grammar = \"NE: {<NPROP>+}\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Kit', u'N'),\n",
       " (u'com', u'PREP'),\n",
       " (u'4', u'NUM'),\n",
       " (u'Pneus', u'NPROP'),\n",
       " (u'de', u'NPROP'),\n",
       " (u'Alta', u'NPROP'),\n",
       " (u'Performance', u'NPROP'),\n",
       " (u'Pirelli', u'NPROP'),\n",
       " (u'Aro', u'NPROP'),\n",
       " (u'16', u'NPROP'),\n",
       " (u'205/55R16', u'NPROP'),\n",
       " (u'Phantom', u'NPROP'),\n",
       " (u'Chegou', u'V'),\n",
       " (u'o', u'ART'),\n",
       " (u'kit', u'N'),\n",
       " (u'que', u'PRO-KS'),\n",
       " (u'junta', u'V'),\n",
       " (u'resist\\xeancia', u'N'),\n",
       " (u'e', u'KC'),\n",
       " (u'conforto', u'N'),\n",
       " (u',', u'PU'),\n",
       " (u'al\\xe9m', u'PREP'),\n",
       " (u'de', u'PREP'),\n",
       " (u'n\\xedveis', u'N'),\n",
       " (u'm\\xe1ximos', u'ADJ'),\n",
       " (u'de', u'PREP'),\n",
       " (u'seguran\\xe7a', u'N'),\n",
       " (u'.', u'PU')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Kit/N\n",
      "  com/PREP\n",
      "  4/NUM\n",
      "  (NE\n",
      "    Pneus/NPROP\n",
      "    de/NPROP\n",
      "    Alta/NPROP\n",
      "    Performance/NPROP\n",
      "    Pirelli/NPROP\n",
      "    Aro/NPROP\n",
      "    16/NPROP\n",
      "    205/55R16/NPROP\n",
      "    Phantom/NPROP)\n",
      "  Chegou/V\n",
      "  o/ART\n",
      "  kit/N\n",
      "  que/PRO-KS\n",
      "  junta/V\n",
      "  resistencia/N\n",
      "  e/KC\n",
      "  conforto/N\n",
      "  ,/PU\n",
      "  alem/PREP\n",
      "  de/PREP\n",
      "  niveis/N\n",
      "  maximos/ADJ\n",
      "  de/PREP\n",
      "  seguranca/N\n",
      "  ./PU)\n"
     ]
    }
   ],
   "source": [
    "print cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'-Captura',\n",
      "     u'-Compatibilidade',\n",
      "     u'-Entrada',\n",
      "     u'-Entrada auxiliar Plug',\n",
      "     u'-Grava',\n",
      "     u'-Sistema 5.1 Canais Prologic',\n",
      "     u'-Tipo',\n",
      "     u'2 Vodkas Sueca Absolut Vanilia',\n",
      "     u'ANVISA',\n",
      "     u\"Assassin 's Creed\",\n",
      "     u'Assassinos',\n",
      "     u'BCAA',\n",
      "     u'BCAA 2400 - 200 C\\xe1psulas',\n",
      "     u'BCAAs',\n",
      "     u'Baunilha',\n",
      "     u'Brasil',\n",
      "     u'CDR-W',\n",
      "     u'Chandon Brut Ros\\xe9',\n",
      "     u'Colorir',\n",
      "     u'Divirta-se',\n",
      "     u'Evie Frye',\n",
      "     u'Fabricante/Fornecedor',\n",
      "     u'Floresta Encantada',\n",
      "     u'Fran\\xe7a -Embalagem',\n",
      "     u'Henry Green',\n",
      "     u'Home Theater Com DVD Lenoxx',\n",
      "     u'Home Theater DVD Player Lenoxx HT723 270W 5.1 Canais USB Fun\\xe7\\xe3o Karaok\\xea Obtenha',\n",
      "     u'Home Theater Lenoxx',\n",
      "     u'Imagens Meramente Ilustrativas BCAA 2400 - 100 C\\xe1psulas',\n",
      "     u'Inglaterra Vitoriana',\n",
      "     u'Isoleucina',\n",
      "     u'Jacob',\n",
      "     u'Jardim Secreto',\n",
      "     u'Jardim Secreto + Floresta Encantada + Reino Animal Entretenimento',\n",
      "     u'Johanna Basford',\n",
      "     u'Kit - Livros de Colorir',\n",
      "     u'Leucina',\n",
      "     u'Livro de Colorir e Ca\\xe7a Tesouro Antiestresse',\n",
      "     u'Livro de Colorir e Ca\\xe7a ao Tesouro Antiestresse',\n",
      "     u'Londres',\n",
      "     u'Michael Fassbender',\n",
      "     u'Millie',\n",
      "     u'Millie Marotta',\n",
      "     u'Morango',\n",
      "     u'M\\xf6et & Chandon',\n",
      "     u'Nitech Nutrition A 100 % Whey da Nitech Nutrition',\n",
      "     u'Nitech Nutrition O BCAA 2400 Nitech Nutrition',\n",
      "     u'Nitech Nutrition Whey Protein Isolate',\n",
      "     u'O Fantasma',\n",
      "     u'Phanthon',\n",
      "     u'Pinot Noir',\n",
      "     u'Pirelli',\n",
      "     u'Pneu Pirelli',\n",
      "     u'Pneus de Alta Performance Pirelli Aro 16 205/55R16 Phantom',\n",
      "     u'Power Picolinato De Cromo A PowerFoods',\n",
      "     u'Power Picolinato de Cromo',\n",
      "     u'Prometheus',\n",
      "     u'P\\xe9rolas rosas -Sabor',\n",
      "     u'RCA',\n",
      "     u'RDC 27',\n",
      "     u'Reino Animal',\n",
      "     u'Revolu\\xe7\\xe3o Industrial',\n",
      "     u'Ros\\xe9',\n",
      "     u'Ros\\xe9 -Aroma',\n",
      "     u'Super Turbo Force Ultra Concentrado 60 C\\xe1psulas - Intlab Super Turbo Force',\n",
      "     u'Templ\\xe1rios',\n",
      "     u'USB',\n",
      "     u'Ubisoft',\n",
      "     u'Uvas -Ingredientes',\n",
      "     u'Valina',\n",
      "     u'Vodkas Sueca Absolut Vanilia 1000ml A Absolut Vanilia',\n",
      "     u'Whey Protein Isolate',\n",
      "     u'X-Men First Class',\n",
      "     u'insulina.Em'])\n"
     ]
    }
   ],
   "source": [
    "entities = set()\n",
    "for tree in cp.parse_sents(sentences):\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NE': \n",
    "            entity = ' '.join([word for word, tag in subtree.leaves()])\n",
    "            entities.add(entity)\n",
    "            \n",
    "from pprint import pprint\n",
    "pprint(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd solution: using CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no corpus available for Portuguese NER training in the python toolkits. However, there is the CONLL corpus available for english.\n",
    "\n",
    "For more information, please follow the tutorial at (http://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb)\n",
    "\n",
    "For Portuguese, there is available a dataset for download at (http://www.linguateca.pt/aval_conjunta/HAREM/harem_ing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
